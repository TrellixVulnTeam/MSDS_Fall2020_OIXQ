---
title: "Test_1 622"
author: "Salma Elshahawy"
date: "10/29/2020"
output:
  html_document:
    code_folding: show
    df_print: paged
    highlight: tango
    number_section: no
    theme: cosmo
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

## Problem statement

Use the dataset you used for HW-1 (Blue/Black)

(A) Run Bagging (ipred package)   

  -- sample with replacement

  -- estimate metrics for a model

  -- repeat as many times as specied and report the average

(B) Run LOOCV (jacknife) for the same dataset

--- iterate over all points

  -- keep one observation as test

  -- train using the rest of the observations

  -- determine test metrics

  -- aggregate the test metrics

end of loop

* find the average of the test metric(s)

* Compare (A), (B) above with the results you obtained in HW-1  and write 3 sentences explaining the observed difference.

## Reading the data 

First we need to construct the data and read it into a dataframe format.

```{r message=FALSE, warning=FALSE}
df <- data.frame(
  X = as.factor(c(5, 5, 5, 5, 5, 5, 19, 19, 19, 19, 19, 19, 35, 35, 35, 35, 35, 35, 51, 51, 51, 51, 51, 51, 55, 55, 55, 55, 55, 55, 63, 63, 63, 63, 63, 63)),
  Y = c("a","b","c","d","e","f","a","b","c","d","e","f","a","b","c","d","e","f","a","b","c","d","e","f","a","b","c","d","e","f","a","b","c","d","e","f"),
  label = c("BLUE","BLACK","BLUE","BLACK","BLACK","BLACK","BLUE","BLUE","BLUE","BLUE","BLACK","BLUE","BLACK","BLACK","BLUE","BLACK","BLACK","BLACK","BLACK","BLACK","BLUE","BLACK","BLACK","BLACK","BLACK","BLACK","BLACK","BLACK","BLACK","BLACK","BLACK","BLUE","BLUE","BLUE","BLUE","BLUE")
)
df
```

## Base line performance of Logistic regression model 

```{r message=FALSE, warning=FALSE}
set.seed(43)
library(caret)
library(ModelMetrics)
library(dplyr)

respCol <- df$label
in_train <- createDataPartition(respCol, p = .70, list = FALSE, times = 1) 
train <- df[in_train,] #training 
test <- df[-in_train,] # test data

# These subsets will help with training and evaluation
training_df_without_label <- train %>% select(-label)
test_df_without_label  <- test %>% select(-label)
training_df_label <- train$label
test_df_label <- test$label

perfALG <- c("LR")
perfAUC = numeric()
perfACC = numeric()
perfTPR = numeric()
perfFPR = numeric()
perfTNR = numeric()
perfFNR = numeric()
```

```{r}
get_lr_yhat <- function(lr_model, data, label_col_name, threshold = 0.5){
  data_levels <- levels(data[[label_col_name]])
  cols_to_keep <- label_col_name != names(data)
  data <- data[,cols_to_keep]
  lr_yhat <- predict(lr_model, data, type = "response")
  lr_yhat <- as.factor(ifelse(lr_yhat <= threshold, data_levels[1], data_levels[2]))
  return(lr_yhat)
}
```


The second will take the the ground truth labels and predicted labels and create metrics for evaluation.


```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
library(kableExtra)
library(caret)
library(e1071)
library(class)
library(ROCR)
lr_model <- glm(label ~ ., data = train, family = "binomial")
training_lr_yhat <- get_lr_yhat(lr_model, train, "label")
```

```{r}
lr_yhat <- get_lr_yhat(lr_model, test, "label")
test_lr_cm <- caret::confusionMatrix(table(test_df_label, lr_yhat))
# capacity_to_generalize <- evaluate_algo(test_df_label, lr_yhat, "LR")

```

```{r}
test_lr_cm
```


```{r message=FALSE, warning=FALSE}
lrProb <- predict(lrFit, newdata = df[-train$Resample1,], type = "response")
lr.class <- unlist(apply(lrPred, 1, which.max))
lr.tb <- table(respCol, lr.class)
lr.cfm <- caret::confusionMatrix(lr.tb)
lr.cfm
# lrPred <- rep("BLACK", length(lrProb))
# lrPred[lrProb > 0.5] = "BLUE"
# lrPred = as.factor(lrPred)
#postResample(lrPred, obs)
# perfAUC <- c(perfAUC, auc(actual = obs, predicted = lrPred))
# perfACC <- c(perfACC, postResample(lrPred, obs)["Accuracy"])
# perfTPR <- c(perfTPR, caret::sensitivity(lrPred, obs))
# perfFPR <- c(perfFPR, 1 - caret::specificity(lrPred, obs))
# perfTNR <- c(perfTNR, caret::specificity(lrPred, obs))
# perfFNR <- c(perfFNR, 1 - caret::sensitivity(lrPred, obs))
# confusionMatrix(lrPred, obs)
# table(lrPred, obs)
# print(lrPred )
```





The `bagging()` function comes from the `ipred package` and we use `nbagg` to control how many iterations to include in the bagged model and `coob = TRUE` indicates to use the `OOB error rate`. By default, `bagging()` uses `rpart::rpart()` for decision tree base learners but other base learners are available. Since bagging just aggregates a base learner, we can tune the base learner parameters as normal.

```{r}
library(ipred)
library(rpart)
# make bootstrapping reproducible
set.seed(210477)

# train bagged model
df_bag1 <- bagging(
  formula = label ~ .,
  data = df,
  nbag = 100,
  coob = TRUE
)

print(df_bag1)
```

```{r}
class_prediction <- predict(object = df_bag1, newdata = df, type = "class")
print(class_prediction)
```

```{r}
df_pred = cbind(df, class_prediction)
df_pred
```







```{r}
library(caret)
confusionMatrix(data = class_prediction, reference = df$label)
```

```{r}
library(pROC)
roc_qda = roc(response=df_pred$label, predictor= factor(df_pred$class_prediction, 
ordered = TRUE), plot=TRUE)
plot(roc_qda, col="red", lwd=3, main="ROC curve using bagging")
# auc_qda<-auc(roc_qda)
```

```{r}
# make bootstrapping reproducible
set.seed(210477)

# train bagged model
df_bag2 <- bagging(
  formula = label ~ .,
  data = df,
  nbag = 200,
  coob = TRUE
)

print(df_bag2)
```


```{r}
class_prediction2 <- predict(object = df_bag2, newdata = df, type = "class")
print(class_prediction2)
```

```{r}
df_pred2 = cbind(df, class_prediction2)
df_pred2
```

```{r}
confusionMatrix(data = class_prediction2, reference = df$label)
```

```{r}
library(pROC)
roc_qda = roc(response=df_pred2$label, predictor= factor(df_pred2$class_prediction2, 
ordered = TRUE), plot=TRUE)
plot(roc_qda, col="red", lwd=3, main="ROC curve using bagging nbag = 200")
# auc_qda<-auc(roc_qda)
```


```{r}
# make bootstrapping reproducible

# train bagged model
df_bag3 <- bagging(
  formula = label ~ .,
  data = df,
  nbag = 20,
  coob = TRUE
)

print(df_bag3)
```


```{r}
class_prediction3 <- predict(object = df_bag3, newdata = df, type = "class")
print(class_prediction3)
```

```{r}
df_pred3 = cbind(df, class_prediction3)
df_pred3
```

```{r}
confusionMatrix(data = class_prediction3, reference = df$label)
```

```{r}
library(pROC)
roc_qda = roc(response=df_pred3$label, predictor= factor(df_pred3$class_prediction3, 
ordered = TRUE), plot=FALSE)
plot(roc_qda, col="red", lwd=3, main="ROC curve using bagging nbag = 20")
# auc_qda<-auc(roc_qda)
```

## Report the average ---? 

```{r}
library(dplyr)
library(tidyr)
class_prediction %>%
  as.data.frame() %>%
  mutate(
    observation = 1:n(),
    actual = df$label) %>%
  tidyr::gather(tree, predicted, -c(observation, actual)) %>%
  group_by(observation) %>%
  mutate(tree = stringr::str_extract(tree, '\\d+') %>% as.numeric()) %>%
  ungroup() %>%
  arrange(observation, tree) %>%
  group_by(observation) %>%
  mutate(avg_prediction = cummean(predicted)) %>%
  group_by(tree) %>%
  summarize(RMSE = RMSE(avg_prediction, actual)) %>%
  ggplot(aes(tree, RMSE)) +
  geom_line() +
  xlab('Number of trees')
```


## LOOCV





