---
title: "assignment_1"
author: "Salma Elshahawy"
date: "10/5/2020"
output:
  pdf_document:
    toc: yes
  html_document:
    code_folding: show
    df_print: paged
    highlight: tango
    number_section: no
    theme: cosmo
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: yes
  word_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
set.seed(41)
library(skimr)
```

## Reading the data 

```{r message=FALSE, warning=FALSE}
df <- data.frame(
  X = as.factor(c(5, 5, 5, 5, 5, 5, 19, 19, 19, 19, 19, 19, 35, 35, 35, 35, 35, 35, 51, 51, 51, 51, 51, 51, 55, 55, 55, 55, 55, 55, 63, 63, 63, 63, 63, 63)),
  Y = c("a","b","c","d","e","f","a","b","c","d","e","f","a","b","c","d","e","f","a","b","c","d","e","f","a","b","c","d","e","f","a","b","c","d","e","f"),
  label = c("BLUE","BLACK","BLUE","BLACK","BLACK","BLACK","BLUE","BLUE","BLUE","BLUE","BLACK","BLUE","BLACK","BLACK","BLUE","BLACK","BLACK","BLACK","BLACK","BLACK","BLUE","BLACK","BLACK","BLACK","BLACK","BLACK","BLACK","BLACK","BLACK","BLACK","BLACK","BLUE","BLUE","BLUE","BLUE","BLUE")
)
df
```

## Data Exploration 

```{r message=FALSE, warning=FALSE}
str(df)
```

```{r message=FALSE, warning=FALSE}
skim(df)
```


```{r message=FALSE, warning=FALSE}
summary(df)
```

## Preparing the data for ML model

```{r message=FALSE, warning=FALSE}
library(caret)
library(ModelMetrics)

respCol <- ncol(df)[[1]]
train <- createDataPartition(df[,respCol], p = .70) # training data
obs <- df[-train$Resample1, respCol] # test data


perfALG <- c("LR","NB", "Knn3", "knn5")
perfAUC = numeric()
perfACC = numeric()
perfTPR = numeric()
perfFPR = numeric()
perfTNR = numeric()
perfFNR = numeric()
```

## Simple Linear regression Model 

```{r message=FALSE, warning=FALSE}
## Logistic Regression (LR) Model
lrFit <- glm(label~., data = df[train$Resample1, ], family = binomial)
summary(lrFit)
```

## Fit the data into LR

```{r message=FALSE, warning=FALSE}
lrProb <- predict(lrFit, newdata = df[-train$Resample1,], type = "response")

lrPred <- rep("BLACK", length(lrProb))
lrPred[lrProb > 0.5] = "BLUE"
lrPred = as.factor(lrPred)
#postResample(lrPred, obs)
perfAUC <- c(perfAUC, auc(actual = obs, predicted = lrPred))
perfACC <- c(perfACC, postResample(lrPred, obs)["Accuracy"])
perfTPR <- c(perfTPR, caret::sensitivity(lrPred, obs))
perfFPR <- c(perfFPR, 1 - caret::specificity(lrPred, obs))
perfTNR <- c(perfTNR, caret::specificity(lrPred, obs))
perfFNR <- c(perfFNR, 1 - caret::sensitivity(lrPred, obs))
table(lrPred, obs)
```

As we can see, we got a confusion matrix with 6 True BLACKS, 3 True BLUES. However, we got 1 False BLUE predicted as BLACK. Over all the performance is not bad and considered a good classifier.


## Naive Bayes Model 

```{r message=FALSE, warning=FALSE}
## Naive Bayes (NB) Model
library(e1071)
nbFit <- naiveBayes(label ~ ., data = df[train$Resample1, ])
#print(nbFit)
nbPred <- predict(nbFit, newdata = df[-train$Resample1,], type = "class")
#postResample(nbPred, obs)
perfAUC <- c(perfAUC, auc(actual = obs, predicted = nbPred))
perfACC <- c(perfACC, postResample(nbPred, obs)["Accuracy"])
perfTPR <- c(perfTPR, caret::sensitivity(nbPred, obs))
perfFPR <- c(perfFPR, 1 - caret::specificity(nbPred, obs))
perfTNR <- c(perfTNR, caret::specificity(nbPred, obs))
perfFNR <- c(perfFNR, 1 - caret::sensitivity(nbPred, obs))
table(nbPred, obs)
```

It surprisingly introduces a higher prediction errors; although NB performs well with text classification and with a small data. 
It gave 6 Trues on BLACKS, 2 Trues on BLUE. However, it gave 2 Falses on BLUES predicted as BLACK. 

## KNN ML Model(k=3)

```{r message=FALSE, warning=FALSE}
## KNN Model
knnFit3 <- knn3(label ~., data = df[train$Resample1,], k = 3)
knnPred3 <- predict(knnFit3, newdata = df[-train$Resample1,], type = "class")
#postResample(knnPred, obs)
perfAUC <- c(perfAUC, auc(actual = obs, predicted = knnPred3))
perfACC <- c(perfACC, postResample(knnPred3, obs)["Accuracy"])
perfTPR <- c(perfTPR, caret::sensitivity(knnPred3, obs))
perfFPR <- c(perfFPR, 1 - caret::specificity(knnPred3, obs))
perfTNR <- c(perfTNR, caret::specificity(knnPred3, obs))
perfFNR <- c(perfFNR, 1 - caret::sensitivity(knnPred3, obs))
table(knnPred3, obs)
```

KNN with K = 3 gave the same performance as Naive Bayes. 

## KNN ML Model (k=5)

```{r message=FALSE, warning=FALSE}
## KNN Model
knnFit5 <- knn3(label ~., data = df[train$Resample1,], k = 5)
knnPred5 <- predict(knnFit5, newdata = df[-train$Resample1,], type = "class")
#postResample(knnPred, obs)
perfAUC <- c(perfAUC, auc(actual = obs, predicted = knnPred5))
perfACC <- c(perfACC, postResample(knnPred5, obs)["Accuracy"])
perfTPR <- c(perfTPR, caret::sensitivity(knnPred5, obs))
perfFPR <- c(perfFPR, 1 - caret::specificity(knnPred5, obs))
perfTNR <- c(perfTNR, caret::specificity(knnPred5, obs))
perfFNR <- c(perfFNR, 1 - caret::sensitivity(knnPred5, obs))
table(knnPred5, obs)
```

Again, changing the k parameter didn't chage much in the overall performance to the classifier, where it gave the same k = 3

## Generate an accuracy comparing table 

```{r message=FALSE, warning=FALSE}
perf <- data.frame(
  ALGO = perfALG,
  AUC = perfAUC,
  ACCURACY = perfACC,
  TPR = perfTPR,
  FPR = perfFPR,
  TNR = perfTNR,
  FNR = perfFNR
)
```


```{r message=FALSE, warning=FALSE}
perf
```

## Summary 

It is hard to determine a clear winner among the classifiers, as multiple runs will select different training and testing data and greatly influence the training and performance of each model from run to run. However, since the dataset is pretty small and small datasets require strong assumption (bias). As it is preferable to use Occam's razor first. The less the assumptions are and the hypothesis is, the better is the results. In our case, Linear regression seems to perform well based on the AUC and ACCURACY.








